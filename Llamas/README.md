First download and unzip the llamas dataset zip (colored or b&w) into the root directory of hawp

Create a temp/ folder in the root directory
Go into that folder and copy all the files from train/ into hawp/temp/

run renameAll.py. This is such that when all the images and data are pulled into the same directory, no data is lost.

cd into that directory and run $find . -mindepth 2 -type f -print -exec mv {} . \;

then type mv * ../data/wireframe/images

run $sbatch create_train.sh to create the jsons for train, val and test for llamas dataset
(here I allocate 2/3 of the data as training data, 1/6 as val and 1/6 as test you can change this in the conditions of create_llamas_jsons.py)

move train.json, val.json and test.json to data/wireframe/

When training use $sbatch run_train.sh

When checking the training loss or validation loss, rename whichever test set you would like to use as the train test set to be train.json in data/wireframe/ and run $sbatch run_train_test.sh

It will output the loss of each epoch on the specified dataset for a sample of 500 batches or 3000 images (batch size of 6) The results of the script should be in the graham out file generated by the script.

If you want to get the val loss, rename train.json to something else temporarily and rename val.json to train.json

If you want to test the set on the test set run $sbatch run_test.sh and get the results in a json format

If you want to test it and see the results plotted run $sbatch run_predict.sh
However you must first place the images you want to predict into figures/ and edit run_predict.sh to use the names of the images you chose

